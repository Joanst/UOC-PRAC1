# -*- coding: utf-8 -*-
"""
@author: Joan i Victor
"""

from bs4 import BeautifulSoup 
import requests
import time 
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys

#Modificar User Agent
headers = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,\
    */*;q=0.8",
    "Accept-Encoding": "gzip, deflate, sdch, br",
    "Accept-Language": "en-US,en;q=0.8",
    "Cache-Control": "no-cache",
    "dnt": "1",
    "Pragma": "no-cache",
    "Upgrade-Insecure-Requests": "1",
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/5\
    37.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36"
}
    
# Web a la que accedim
pagina="https://www.3cat.cat/tv3/cuines/receptes/"

# Obtenim la pàgina sencera i creem un objecte beautiful soup per treballar-hi
page= requests.get(pagina, headers=headers)
soupPage= BeautifulSoup(page.content, features="html.parser")

# Obtenir el numero de pàgines de receptes
ultimaPagina = soupPage.find("p", class_="numeracio")
ultimaPagina = ultimaPagina.text.split(" ")[3]
ultimaPagina = int(ultimaPagina)
print("Tenim un total de", ultimaPagina, "pagines! \n")

def extreure_receptes(website):
    receptes = website.find_all("div", class_ = "M-destacat cuines T-cuinesTema")

    for recepta in receptes:
        print(recepta)
        nom = recepta.find("a", {"class": "titol--a"})
        img = recepta.find("img", class_="foto")
        
        pagina_url = "https://www.3cat.cat" + nom["href"]
        pagina_img = img["src"]
        urls.append((pagina_url, pagina_img))

urls = []
i = 1

driver = webdriver.Chrome()
driver.get(pagina)
time.sleep(1)

cookies = driver.find_element(By.ID, "didomi-notice-disagree-button")
cookies.click()

while i <= 1:# ultimaPagina:
    print("Estem en la pagina:", i, "\n")
    website = BeautifulSoup(driver.page_source, features="html.parser")
    extreure_receptes(website)
    if i != ultimaPagina:
        seguent = driver.find_element(By.CSS_SELECTOR, "li.R-seg a[data-toggle='tab']")
        seguent.click()
        time.sleep(2)
    i += 1
    
driver.quit()

print(len(urls))

with open('urls.txt', 'w') as f:
    for url, img in urls:
        f.write(f"{url}, ")
        f.write(f"{img}\n")
       
for linkRecepta, img_url in urls:
    page = requests.get(linkRecepta, headers=headers)
    soupPage = BeautifulSoup(page.content, features="html.parser")
    
    # Obtenim el nom de la recepta
    nomRecepta = soupPage.find_all("h1")[1]
    nomRecepta = nomRecepta.string
    print(nomRecepta)
    
    # Obtenim els tags
    div_tags = soupPage.find("div", class_ = "llistat-tags")
    a_tags = div_tags.find_all("a")
    Etiquetes = [tag.string for tag in a_tags]
    print(Etiquetes)
    
    # Obtenim la info basica
    div_info = soupPage.find("div", class_ = "span4 informacio-basica")
    li_info = div_info.find_all("li")
    try:
        Dificultat = li_info[2].get_text().split()[1]
    except:
        Dificultat = None
    try:
        Temps = li_info[3].get_text().split(":")[1].replace("\n", "").strip()
    except:
        Temps = None
    try:
        Dieta = li_info[4].get_text().split()[1]
    except:
        Dieta = None
    print(Dificultat)
    print(Temps)
    print(Dieta)
    
    # Obtenim els ingredients
    div_ingredients = soupPage.find("div", class_ = "ingredients")
    li_ingredients = div_ingredients.find_all("p")
    Ingredients = [ingredient.string for ingredient in li_ingredients]
    print(Ingredients)
    
    # Obtenim la preparacio
    preparacio = soupPage.find("h2", string="PREPARACIÓ")
    pasos = preparacio.find_next_sibling().find_all(["li", "p"])
    Preparacio = [pas.string for pas in pasos]
    print(Preparacio)
    
    # Obtenim la fotografia
    Imatge = img_url
    print(Imatge)
